{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "import io\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaskar.gurram/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: This function is deprecated. Please call randint(1000000.0, 1000000000.0 + 1) instead\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "metadata_path = \"data/preprocessed_glove.metadata.pkl\"\n",
    "emb_path = \"data/preprocessed_glove.emb.npy\"\n",
    "tokenized_trn_json_path = 'data/train-v1.1.tokenized.split.json'\n",
    "tokenized_dev_json_path = 'data/dev-v1.1.tokenized.split.json'\n",
    "        \n",
    "\n",
    "device = None                      # 'cpu' / 'gpu<index>'\n",
    "save_freq = None                   # how often to save model (in epochs); None for only after best EM/F1 epochs\n",
    "test_json_path = None              # path of test set JSON\n",
    "pred_json_path = None              # path of test predictions JSON\n",
    "tst_load_model_path = None         # path of trained model data, used for producing test set predictions\n",
    "tst_split = True                   # whether to split hyphenated unknown words of test set, see setup.py\n",
    "seed = np.random.random_integers(1e6, 1e9)\n",
    "max_ans_len = 30                   # maximal answer length, answers of longer length are discarded\n",
    "emb_dim = 300                      # dimension of word embeddings\n",
    "learn_single_unk = True           # whether to have a single tunable word embedding for all unknown words # (or multiple fixed random ones)\n",
    "init_scale = 5e-3                  # uniformly random weights are initialized in [-init_scale, +init_scale]\n",
    "learning_rate = 1e-3\n",
    "lr_decay = 0.95\n",
    "lr_decay_freq = 5000               # frequency with which to decay learning rate, measured in updates\n",
    "max_grad_norm = 10                 # gradient clipping\n",
    "ff_dims = [100]                    # dimensions of hidden FF layers\n",
    "ff_drop_x = 0.2                    # dropout rate of FF layers\n",
    "batch_size = 32\n",
    "max_num_epochs = 150               # max number of epochs to train for\n",
    "num_bilstm_layers = 2              # number of BiLSTM layers, where BiLSTM is applied\n",
    "hidden_dim = 100                   # dimension of hidden state of each uni-directional LSTM\n",
    "lstm_drop_h = 0.1                  # dropout rate for recurrent hidden state of LSTM\n",
    "lstm_drop_x = 0.4                  # dropout rate for inputs of LSTM\n",
    "lstm_couple_i_and_f = True         # customizable LSTM configuration, see base/model.py\n",
    "lstm_learn_initial_state = False\n",
    "lstm_tie_x_dropout = True\n",
    "lstm_sep_x_dropout = False\n",
    "lstm_sep_h_dropout = False\n",
    "lstm_w_init = 'uniform'\n",
    "lstm_u_init = 'uniform'\n",
    "lstm_forget_bias_init = 'uniform'\n",
    "default_bias_init = 'uniform'\n",
    "extra_drop_x = 0                   # dropout rate at an extra possible place\n",
    "q_aln_ff_tie = True                # whether to tie the weights of the FF over question and the FF over passage\n",
    "sep_stt_end_drop = True            # whether to have separate dropout masks for span start and # span end representations\n",
    "adam_beta1 = 0.9                   # see base/optimizer.py\n",
    "adam_beta2 = 0.999\n",
    "adam_eps = 1e-8\n",
    "objective = 'span_multinomial'     # 'span_multinomial': multinomial distribution over all spans\n",
    "                                   # 'span_binary':      logistic distribution per span\n",
    "                                   # 'span_endpoints':   two multinomial distributions, over span start and end\n",
    "ablation = None    \n",
    "log_file_name = 'testingLog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=log_file_name,level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SquadDatasetVectorized = namedtuple('SquadDatasetVectorized', [\n",
    "    'qtn_ctx',\n",
    "    'qtn_ctx_lens',\n",
    "    'qtns',\n",
    "    'qtn_lens',\n",
    "    'anss'\n",
    "])\n",
    "\n",
    "WordEmbData = namedtuple('WordEmbData', [\n",
    "  'word_emb',                 # float32 (num words, emb dim)\n",
    "  'str_to_word',              # map word string to word index\n",
    "  'first_known_word',         # words found in GloVe are at positions [first_known_word, first_unknown_word)\n",
    "  'first_unknown_word',       # words not found in GloVe are at positions [first_unknown_word, first_unallocated_word)\n",
    "  'first_unallocated_word'    # extra random embeddings\n",
    "])\n",
    "\n",
    "TokenizedText = namedtuple('TokenizedText', [\n",
    "    'text',  # original text string\n",
    "    'tokens',  # list of parsed tokens\n",
    "    'originals',  # list of original tokens (may differ from parsed ones)\n",
    "    'whitespace_afters',  # list of whitespace strings, each appears after corresponding original token in original text\n",
    "])\n",
    "\n",
    "SquadArticle = namedtuple('SquadArticle', [\n",
    "    'art_title_str'\n",
    "])\n",
    "\n",
    "SquadContext = namedtuple('SquadContext', [\n",
    "    'art_idx',\n",
    "    'tokenized'  # TokenizedText of context's text\n",
    "])\n",
    "\n",
    "SquadQuestion = namedtuple('SquadQuestion', [\n",
    "    'ctx_idx',\n",
    "    'qtn_id',\n",
    "    'tokenized',  # TokenizedText of question's text\n",
    "    'ans_texts',  # list of (possibly multiple) answer text strings\n",
    "    'ans_word_idxs'  # list where each entry is either a (answer start word index, answer end word index) tuple\n",
    "    # or None for answers that we failed to parse\n",
    "])\n",
    "\n",
    "\n",
    "class SquadDatasetTabular(object):\n",
    "    def __init__(self):\n",
    "        self.arts = []  # SquadArticle objects\n",
    "        self.ctxs = []  # SquadContext objects\n",
    "        self.qtns = []  # SquadQuestion objects\n",
    "\n",
    "    def new_article(self, art_title_str):\n",
    "        self.arts.append(SquadArticle(art_title_str))\n",
    "        return len(self.arts) - 1\n",
    "\n",
    "    def new_context(self, art_idx, ctx_tokenized):\n",
    "        self.ctxs.append(SquadContext(art_idx, ctx_tokenized))\n",
    "        return len(self.ctxs) - 1\n",
    "\n",
    "    def new_question(self, ctx_idx, qtn_id, qtn_tokenized, ans_texts, ans_word_idxs):\n",
    "        self.qtns.append(\n",
    "            SquadQuestion(ctx_idx, qtn_id, qtn_tokenized, ans_texts, ans_word_idxs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_word_emb_data():\n",
    "    with open(metadata_path, 'rb') as f:\n",
    "        str_to_word, first_known_word, first_unknown_word, first_unallocated_word = pkl.load(f)\n",
    "    with open(emb_path, 'rb') as f:\n",
    "        word_emb = np.load(f)\n",
    "    word_emb_data = WordEmbData(word_emb, str_to_word, first_known_word, first_unknown_word, first_unallocated_word)\n",
    "    return word_emb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _make_tabular_dataset(tokenized_json_path, word_strs, has_answers, max_ans_len=None):\n",
    "    tabular = SquadDatasetTabular()\n",
    "    \n",
    "    num_questions = 0\n",
    "    num_answers = 0\n",
    "    num_invalid_answers = 0\n",
    "    num_long_answers = 0\n",
    "    num_invalid_questions = 0\n",
    "\n",
    "    answers_per_question_counter = Counter()\n",
    "    with io.open(tokenized_json_path, 'r', encoding='utf-8') as f:\n",
    "        j = json.load(f)\n",
    "        data = j['data']\n",
    "        for article in data:\n",
    "            art_title_str = article['title']\n",
    "            art_idx = tabular.new_article(art_title_str)\n",
    "\n",
    "            paragraphs = article['paragraphs']\n",
    "            for paragraph in paragraphs:\n",
    "                ctx_str = paragraph['context']\n",
    "                ctx_tokens = paragraph['tokens']\n",
    "                word_strs.update(ctx_tokens)\n",
    "                ctx_originals = paragraph['originals']\n",
    "                ctx_whitespace_afters = paragraph['whitespace_afters']\n",
    "                ctx_tokenized = TokenizedText(ctx_str, ctx_tokens, ctx_originals, ctx_whitespace_afters)\n",
    "                ctx_idx = tabular.new_context(art_idx, ctx_tokenized)\n",
    "\n",
    "                qas = paragraph['qas']\n",
    "                for qa in qas:\n",
    "                    num_questions += 1\n",
    "                    qtn_id = qa['id']\n",
    "\n",
    "                    qtn_str = qa['question']\n",
    "                    qtn_tokens = qa['tokens']\n",
    "                    word_strs.update(qtn_tokens)\n",
    "                    qtn_originals = qa['originals']\n",
    "                    qtn_whitespace_afters = qa['whitespace_afters']\n",
    "                    qtn_tokenized = TokenizedText(qtn_str, qtn_tokens, qtn_originals, qtn_whitespace_afters)\n",
    "\n",
    "                    ans_texts = []\n",
    "                    ans_word_idxs = []\n",
    "                    if has_answers:\n",
    "                        answers = qa['answers']\n",
    "                        \n",
    "                        for answer in answers:\n",
    "                            num_answers += 1\n",
    "                            ans_text = answer['text']\n",
    "                            ans_texts.append(ans_text)\n",
    "                            if not answer['valid']:\n",
    "                                ans_word_idxs.append(None)\n",
    "                                num_invalid_answers += 1\n",
    "                                continue\n",
    "                            ans_start_word_idx = answer['start_token_idx']\n",
    "                            ans_end_word_idx = answer['end_token_idx']\n",
    "                            if max_ans_len and ans_end_word_idx - ans_start_word_idx + 1 > max_ans_len:\n",
    "                                ans_word_idxs.append(None)\n",
    "                                num_long_answers += 1\n",
    "                            else:\n",
    "                                ans_word_idxs.append((ans_start_word_idx, ans_end_word_idx))\n",
    "                        answers_per_question_counter[len(ans_texts)] += 1  # this counts also invalid answers\n",
    "                        num_invalid_questions += 1 if all(ans is None for ans in ans_word_idxs) else 0\n",
    "\n",
    "                    tabular.new_question(ctx_idx, qtn_id, qtn_tokenized, ans_texts, ans_word_idxs)\n",
    "    return tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _contract_word_emb_data(old_word_emb_data, word_strs, is_single_unk):\n",
    "    old_word_emb, old_str_to_word, old_first_known_word, old_first_unknown_word, old_first_unallocated_word = \\\n",
    "        old_word_emb_data\n",
    "\n",
    "    known_word_strs = []\n",
    "    unknown_word_strs = []\n",
    "    for word_str in word_strs:\n",
    "        if word_str in old_str_to_word and old_str_to_word[word_str] < old_first_unknown_word:\n",
    "            known_word_strs.append(word_str)\n",
    "        else:\n",
    "            unknown_word_strs.append(word_str)\n",
    "\n",
    "    str_to_word = {}\n",
    "    emb_size = old_first_known_word + (len(known_word_strs) + 1 if is_single_unk else len(word_strs))\n",
    "    word_emb = np.zeros((emb_size, old_word_emb.shape[1]), dtype=np.float32)\n",
    "\n",
    "    for i, word_str in enumerate(known_word_strs):\n",
    "        word = old_first_known_word + i\n",
    "        str_to_word[word_str] = word\n",
    "        word_emb[word, :] = old_word_emb[old_str_to_word[word_str]]\n",
    "\n",
    "    first_unknown_word = old_first_known_word + len(known_word_strs)\n",
    "\n",
    "    if is_single_unk:\n",
    "        for word_str in unknown_word_strs:\n",
    "            str_to_word[word_str] = first_unknown_word\n",
    "    else:\n",
    "        num_new_unks = 0\n",
    "        for i, word_str in enumerate(unknown_word_strs):\n",
    "            word = first_unknown_word + i\n",
    "            str_to_word[word_str] = word\n",
    "            if word_str in old_str_to_word:\n",
    "                word_emb[word, :] = old_word_emb[old_str_to_word[word_str]]\n",
    "            else:\n",
    "                if old_first_unallocated_word + num_new_unks >= len(old_word_emb):\n",
    "                    sys.exit(1)\n",
    "                word_emb[word, :] = old_word_emb[old_first_unallocated_word + num_new_unks]\n",
    "                num_new_unks += 1\n",
    "    return WordEmbData(word_emb, str_to_word, old_first_known_word, first_unknown_word, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _make_vectorized_dataset(tabular, word_emb_data):\n",
    "    num_ctxs = len(tabular.ctxs)\n",
    "    num_qtns = len(tabular.qtns)\n",
    "    max_ctx_len = max(len(ctx.tokenized.tokens) for ctx in tabular.ctxs)\n",
    "    max_qtn_len = max(len(qtn.tokenized.tokens) for qtn in tabular.qtns)\n",
    "\n",
    "    all_ctxs = np.zeros((num_ctxs, max_ctx_len), dtype=np.int32)\n",
    "    all_ctx_lens = np.zeros(num_ctxs, dtype=np.int32)\n",
    "    qtns = np.zeros((num_qtns, max_qtn_len), dtype=np.int32)\n",
    "    qtn_lens = np.zeros(num_qtns, dtype=np.int32)\n",
    "    qtn_ctx_idxs = np.zeros(num_qtns, dtype=np.int32)\n",
    "    \n",
    "    qtn_ctx = np.zeros((num_qtns, max_ctx_len), dtype=np.int32)\n",
    "    qtn_ctx_lens = np.zeros(num_qtns, dtype=np.int32)\n",
    "    qtn_ans_inds = np.zeros(num_qtns, dtype=np.int32)\n",
    "    anss = np.zeros((num_qtns, 2), dtype=np.int32)\n",
    "\n",
    "    for ctx_idx, ctx in enumerate(tabular.ctxs):\n",
    "        ctx_words = [word_emb_data.str_to_word[word_str] for word_str in ctx.tokenized.tokens]\n",
    "        all_ctxs[ctx_idx, :len(ctx_words)] = ctx_words\n",
    "        all_ctx_lens[ctx_idx] = len(ctx_words)\n",
    "\n",
    "    \n",
    "    for qtn_idx, qtn in enumerate(tabular.qtns):\n",
    "        qtn_words = [word_emb_data.str_to_word[word_str] for word_str in qtn.tokenized.tokens]\n",
    "        qtns[qtn_idx, :len(qtn_words)] = qtn_words\n",
    "        qtn_lens[qtn_idx] = len(qtn_words)\n",
    "        qtn_ctx[qtn_idx] = all_ctxs[qtn.ctx_idx]\n",
    "        qtn_ctx_lens[qtn_idx] = all_ctx_lens[qtn.ctx_idx]\n",
    "        ans = next((ans for ans in qtn.ans_word_idxs if ans), None) if qtn.ans_word_idxs else None\n",
    "        if ans:\n",
    "            ans_start_word_idx, ans_end_word_idx = ans\n",
    "            anss[qtn_idx] = [ans_start_word_idx, ans_end_word_idx]\n",
    "            qtn_ans_inds[qtn_idx] = 1\n",
    "        else:\n",
    "            qtn_ans_inds[qtn_idx] = 0\n",
    "        \n",
    "    return SquadDatasetVectorized(qtn_ctx, qtn_ctx_lens, qtns, qtn_lens, anss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DatasetLoader(Dataset):\n",
    "    def __init__(self, ctx, ctx_lens, qtns, qtn_lens, anss):\n",
    "        self.ctx = ctx\n",
    "        self.ctx_lens = ctx_lens\n",
    "        self.qtns = qtns\n",
    "        self.qtn_lens = qtn_lens\n",
    "        self.anss = anss\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.qtns.shape[0]\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.ctx[key], self.ctx_lens[key], self.qtns[key], self.qtn_lens[key],self.anss[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _np_ans_word_idxs_to_ans_idx(ans_start_word_idx, ans_end_word_idx, max_ans_len):\n",
    "    # all arguments are concrete ints\n",
    "    assert ans_end_word_idx - ans_start_word_idx + 1 <= max_ans_len\n",
    "    return ans_start_word_idx * max_ans_len + (ans_end_word_idx - ans_start_word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(batch):\n",
    "    ctxs = []\n",
    "    ctx_lens = []\n",
    "    qtns = []\n",
    "    qtn_lens = []\n",
    "    anss = []\n",
    "    \n",
    "    for datum in batch:\n",
    "        ctxs.append(datum[0])\n",
    "        ctx_lens.append(datum[1])\n",
    "        qtns.append(datum[2])\n",
    "        qtn_lens.append(datum[3])\n",
    "        anss.append(datum[4])\n",
    "\n",
    "    max_question_length = int(max(qtn_lens))\n",
    "    max_ctx_length = int(max(ctx_lens))\n",
    "    n_samples = len(qtns)\n",
    "\n",
    "    contexts = torch.zeros(max_ctx_length, n_samples).long()\n",
    "    contexts_mask = torch.zeros(max_ctx_length, n_samples)\n",
    "    contexts_lens = torch.zeros(n_samples,).long()\n",
    "    \n",
    "    questions = torch.zeros(max_question_length, n_samples).long()\n",
    "    questions_mask = torch.zeros(max_question_length, n_samples)\n",
    "    final_answers = torch.zeros(n_samples, 2).long()\n",
    "    final_y = torch.zeros(n_samples,).long()\n",
    "    \n",
    "    for idx, [ctx, ctx_len, qtn, qtn_len, ans] in enumerate(zip(ctxs, ctx_lens, qtns, qtn_lens, anss)):\n",
    "        contexts[:ctx_len, idx] = torch.from_numpy(ctx[:ctx_len]).long()\n",
    "        questions[:qtn_len, idx] = torch.from_numpy(qtn[:qtn_len]).long()\n",
    "        contexts_mask[:ctx_len, idx] = 1.\n",
    "        questions_mask[:qtn_len, idx] = 1.\n",
    "        final_answers[idx] = torch.from_numpy(ans)\n",
    "        contexts_lens[idx] = int(ctx_len)\n",
    "        final_y[idx] = int(_np_ans_word_idxs_to_ans_idx(ans[0], ans[1], max_ans_len))\n",
    "        \n",
    "    return contexts, contexts_mask, questions, questions_mask, final_answers, contexts_lens, final_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_loc, dev_loc, batch_size):\n",
    "    word_emb_data = read_word_emb_data()\n",
    "    word_strs = set()\n",
    "    \n",
    "    trn_tab_ds = _make_tabular_dataset(train_loc, word_strs, has_answers=True, max_ans_len=max_ans_len)\n",
    "    \n",
    "    dev_tab_ds = _make_tabular_dataset(dev_loc, word_strs, has_answers=True, max_ans_len=max_ans_len)\n",
    "\n",
    "    word_emb_data = _contract_word_emb_data(word_emb_data, word_strs, learn_single_unk)\n",
    "    \n",
    "    trn_vec_ds = _make_vectorized_dataset(trn_tab_ds, word_emb_data)\n",
    "    \n",
    "    \n",
    "    dev_vec_ds = _make_vectorized_dataset(dev_tab_ds, word_emb_data)\n",
    "    \n",
    "    train_dataset = DatasetLoader(trn_vec_ds.qtn_ctx, trn_vec_ds.qtn_ctx_lens, trn_vec_ds.qtns, trn_vec_ds.qtn_lens, trn_vec_ds.anss)\n",
    "    dev_dataset = DatasetLoader(dev_vec_ds.qtn_ctx, dev_vec_ds.qtn_ctx_lens, dev_vec_ds.qtns, dev_vec_ds.qtn_lens, dev_vec_ds.anss)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                              shuffle=True,\n",
    "                              batch_size=batch_size,\n",
    "                              collate_fn=prepare_data\n",
    "                              )\n",
    "    dev_loader = DataLoader(dataset=dev_dataset,\n",
    "                            shuffle=False,\n",
    "                            batch_size=batch_size)\n",
    "                            #collate_fn=prepare_data)\n",
    "    return train_loader, dev_loader, word_emb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "train_data, dev_data, word_emb_data = load_data(tokenized_trn_json_path, tokenized_dev_json_path, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb = Variable(torch.from_numpy(word_emb_data.word_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, nin, hidden_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        if torch.cuda.is_available():\n",
    "            self.linear_f = nn.Linear(nin + hidden_size, hidden_size).cuda()\n",
    "            self.linear_i = nn.Linear(nin + hidden_size, hidden_size).cuda()\n",
    "            self.linear_ctilde = nn.Linear(nin + hidden_size, hidden_size).cuda()\n",
    "            self.linear_o = nn.Linear(nin + hidden_size, hidden_size).cuda()\n",
    "\n",
    "        else:\n",
    "            self.linear_f = nn.Linear(nin + hidden_size, hidden_size)\n",
    "            self.linear_i = nn.Linear(nin + hidden_size, hidden_size)\n",
    "            self.linear_ctilde = nn.Linear(nin + hidden_size, hidden_size)\n",
    "            self.linear_o = nn.Linear(nin + hidden_size, hidden_size)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        hidden, c = self.init_hidden(x.size(1))\n",
    "\n",
    "        def step(emb, hid, c_t_old, mask_cur):\n",
    "            combined = torch.cat((hid, emb), 1)\n",
    "\n",
    "            f = F.sigmoid(self.linear_f(combined))\n",
    "            i = F.sigmoid(self.linear_i(combined))\n",
    "            o = F.sigmoid(self.linear_o(combined))\n",
    "            c_tilde = F.tanh(self.linear_ctilde(combined))\n",
    "\n",
    "            c_t = f * c_t_old + i * c_tilde\n",
    "            c_t = mask_cur[:, None] * c_t + (1. - mask_cur)[:, None] * c_t_old\n",
    "\n",
    "            hid_new = o * F.tanh(c_t)\n",
    "            hid_new = mask_cur[:, None] * hid_new + (1. - mask_cur)[:, None] * hid\n",
    "\n",
    "            return hid_new, c_t, i\n",
    "\n",
    "        h_hist = []\n",
    "        i_hist = []\n",
    "        for i in range(x.size(0)):\n",
    "            hidden, c, i = step(x[i].squeeze(), hidden, c, mask[i])\n",
    "            h_hist.append(hidden[None, :, :])\n",
    "            i_hist.append(i[None, :, :])\n",
    "\n",
    "        return torch.cat(h_hist), torch.cat(i_hist)\n",
    "\n",
    "    def init_hidden(self, bat_size):\n",
    "        if torch.cuda.is_available():\n",
    "            h0 = Variable(torch.zeros(bat_size, self.hidden_size).cuda())\n",
    "            c0 = Variable(torch.zeros(bat_size, self.hidden_size).cuda())\n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(bat_size, self.hidden_size))\n",
    "            c0 = Variable(torch.zeros(bat_size, self.hidden_size))\n",
    "        return h0, c0\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        lin_layers = [self.linear_f, self.linear_i, self.linear_ctilde, self.linear_o]\n",
    "\n",
    "        for layer in lin_layers:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            layer.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAModel(nn.Module):\n",
    "    def __init__(self, word_embeddings, emb_dim, hidden_dim):\n",
    "        super(QAModel, self).__init__()\n",
    "        self.LSTM1 = LSTM(emb_dim, hidden_dim)\n",
    "        self.LSTM1_rev = LSTM(emb_dim, hidden_dim)\n",
    "        \n",
    "        self.LSTM2 = LSTM(2*hidden_dim, hidden_dim)\n",
    "        self.LSTM2_rev = LSTM(2*hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.ff_dims = 100\n",
    "        self.p_start_dim = emb_dim + 2 * hidden_dim +emb_dim\n",
    "        \n",
    "        self.p_LSTM1 = LSTM(self.p_start_dim, hidden_dim)\n",
    "        self.p_LSTM1_rev = LSTM(self.p_start_dim, hidden_dim)\n",
    "        \n",
    "        self.p_LSTM2 = LSTM(2*hidden_dim, hidden_dim)\n",
    "        self.p_LSTM2_rev = LSTM(2*hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.word_emb = word_emb\n",
    "        \n",
    "        \n",
    "        self.linear1 = nn.Linear(2*hidden_dim, self.ff_dims)\n",
    "        self.linear_w = nn.Linear(self.ff_dims, 1, bias = False)\n",
    "        self.linear_q_aligned = nn.Linear(emb_dim, self.ff_dims, bias = False)\n",
    "        self.linear_ans_start = nn.Linear(2*hidden_dim, self.ff_dims)\n",
    "        self.linear_ans_end = nn.Linear(2*hidden_dim, self.ff_dims, bias = False)\n",
    "        self.linear_span = nn.Linear(self.ff_dims, 1, bias = False)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, contexts, contexts_mask, questions, questions_mask, anss, contexts_lens):\n",
    "        n_timesteps_cntx = contexts.size(0)\n",
    "        n_timesteps_quest = questions.size(0)\n",
    "        n_samples = contexts.size(1)\n",
    "        \n",
    "        emb_cntx = self.word_emb[contexts.view(-1)].view(n_timesteps_cntx, n_samples, emb_dim)\n",
    "        emb_quest = self.word_emb[questions.view(-1)].view(n_timesteps_quest, n_samples, emb_dim)\n",
    "        \n",
    "        q_indep = self.compute_q_indep(emb_quest, questions_mask, emb_cntx.size(0))\n",
    "        q_align = self.compute_q_aligned(emb_cntx,emb_quest,contexts_mask,questions_mask)\n",
    "        \n",
    "        p_star = torch.cat((emb_cntx,q_indep,q_align),2)\n",
    "\n",
    "        passage_level = self.sequence_encoder(p_star, contexts_mask, self.p_LSTM1, self.p_LSTM1_rev, self.p_LSTM2, self.p_LSTM2_rev)\n",
    "        \n",
    "        loss, acc, sum_acc, sum_loss = self.compute_answer(passage_level[2], passage_level[2], contexts_lens, batch_size, anss)\n",
    "        return loss, acc, sum_acc, sum_loss\n",
    "        \n",
    "    \n",
    "    def sequence_encoder(self, inp, mask, lstm1, lstm_rev1, lstm2, lstm_rev2):\n",
    "        reverse_emb = self.reverseTensor(inp)\n",
    "        reverse_mask = self.reverseTensor(mask)\n",
    "\n",
    "        #  LSTM1\n",
    "        seq1 = lstm1(inp, mask)\n",
    "        seq_reverse1 = lstm_rev1(reverse_emb, reverse_mask)\n",
    "        \n",
    "        inp_seq2 = torch.cat((seq1[0], self.reverseTensor(seq_reverse1[0])), len(seq1[0].size()) - 1)\n",
    "        reverse_inp_seq2 = self.reverseTensor(inp_seq2)\n",
    "\n",
    "        #  LSTM2\n",
    "        seq2 = lstm2(inp_seq2, mask)\n",
    "        seq_reverse2 = lstm_rev2(reverse_inp_seq2, reverse_mask)\n",
    "        \n",
    "        cat_seq2 = torch.cat((seq2[0], self.reverseTensor(seq_reverse2[0])), len(seq2[0].size()) - 1)\n",
    "        return seq2, seq_reverse2, cat_seq2\n",
    "    \n",
    "        \n",
    "    def compute_q_indep(self, q_emb, q_mask, max_p_len):\n",
    "        encoder_out = self.sequence_encoder(q_emb, q_mask, self.LSTM1, self.LSTM1_rev, self.LSTM2, self.LSTM2_rev)\n",
    "        q_indep_h = encoder_out[2]\n",
    "        q_indep_ff = self.linear1(q_indep_h)\n",
    "        q_indep_scores = self.linear_w(q_indep_ff)\n",
    "        \n",
    "        q_indep_weights = self.softmax_columns_with_mask(q_indep_scores.squeeze(), q_mask)  # (max_q_len, batch_size)\n",
    "        q_indep = torch.sum(q_indep_weights.unsqueeze(2) * q_indep_h, dim=0)  # (batch_size, 2*hidden_dim)\n",
    "\n",
    "        q_indep_repeated = torch.cat([q_indep.unsqueeze(0)] * max_p_len)\n",
    "        \n",
    "        return q_indep_repeated\n",
    "    \n",
    "    def compute_q_aligned(self, p_emb, q_emb, p_mask, q_mask):\n",
    "        q_align_ff_p = self.linear_q_aligned(p_emb)\n",
    "        q_align_ff_q = self.linear_q_aligned(q_emb)\n",
    "        \n",
    "        q_align_ff_p_shuffled = q_align_ff_p.permute(1, 0, 2)  # (batch_size, max_p_len, ff_dim)\n",
    "        q_align_ff_q_shuffled = q_align_ff_q.permute(1, 2, 0)\n",
    "        q_align_scores = torch.bmm(q_align_ff_p_shuffled,q_align_ff_q_shuffled)\n",
    "        \n",
    "        p_mask_shuffled = p_mask.unsqueeze(2).permute(1, 0, 2)\n",
    "        q_mask_shuffled = q_mask.unsqueeze(2).permute(1, 2, 0)\n",
    "        pq_mask = torch.bmm(p_mask_shuffled, q_mask_shuffled)\n",
    "        \n",
    "        q_align_weights = self.softmax_depths_with_mask(q_align_scores, pq_mask) \n",
    "        q_emb_shuffled = q_emb.permute(1, 0, 2)\n",
    "        q_align = torch.bmm(q_align_weights, q_emb_shuffled)\n",
    "        q_align_shuffled = q_align.permute(1, 0, 2)\n",
    "        return q_align_shuffled\n",
    "        \n",
    "    def reverseTensor(self, tensor):\n",
    "        idx = [i for i in range(tensor.size(0) - 1, -1, -1)]\n",
    "        if torch.cuda.is_available():\n",
    "            idx = Variable(torch.LongTensor(idx).cuda())\n",
    "        else:\n",
    "            idx = Variable(torch.LongTensor(idx))\n",
    "        inverted_tensor = tensor.index_select(0, idx)\n",
    "        return inverted_tensor\n",
    "    \n",
    "    def compute_answer(self,p_level_h_for_stt, p_level_h_for_end, p_lens, batch_size, anss):\n",
    "        max_p_len = p_level_h_for_stt.size(0)\n",
    "        p_stt_lin =self.linear_ans_start(p_level_h_for_stt)\n",
    "        p_end_lin =self.linear_ans_end(p_level_h_for_end)\n",
    "        span_lin_reshaped, span_masks_reshaped = self._span_sums(p_stt_lin, p_end_lin, p_lens, max_p_len, batch_size, self.ff_dims, max_ans_len)\n",
    "        span_ff_reshaped = F.relu(span_lin_reshaped)  # (batch_size, max_p_len*max_ans_len, ff_dim)\n",
    "        span_scores_reshaped = self.linear_span(span_ff_reshaped).squeeze()\n",
    "        xents, accs, a_hats = self._span_multinomial_classification(span_scores_reshaped, span_masks_reshaped, anss)\n",
    "        loss = xents.mean()\n",
    "        acc = accs.mean()\n",
    "        sum_acc = accs.sum()\n",
    "        sum_loss = loss.sum()\n",
    "        return loss, acc, sum_acc, sum_loss\n",
    "        \n",
    "    def softmax_columns_with_mask(self, x, mask, allow_none=False):\n",
    "        assert len(x.size()) == 2\n",
    "        assert len(mask.size()) == 2\n",
    "        # for numerical stability\n",
    "\n",
    "        x = x*mask\n",
    "        x = x - x.min(dim=0, keepdim=True)[0]\n",
    "        x = x*mask\n",
    "        x = x - x.max(dim=0, keepdim=True)[0]\n",
    "        e_x = mask * torch.exp(x)\n",
    "        sums = e_x.sum(dim=0, keepdim=True)\n",
    "        if allow_none:\n",
    "            sums += torch.eq(sums, 0)\n",
    "        y = e_x / sums\n",
    "        return y\n",
    "    \n",
    "    def _span_multinomial_classification(self, x, x_mask, y):\n",
    "        # x       float32 (batch_size, num_classes)   scores i.e. logits\n",
    "        # x_mask  int32   (batch_size, num_classes)   score masks (each sample has a variable number of classes)\n",
    "        # y       int32   (batch_size,)               target classes i.e. ground truth answers (given as class indices)\n",
    "        assert len(x.size()) == len(x_mask.size()) == 2\n",
    "        assert len(y.size()) == 1\n",
    "\n",
    "        # substracting min needed since all non masked-out elements of a row may be negative.\n",
    "        x = x * x_mask\n",
    "        x = x - x.min(dim=0, keepdim=True)[0]  # (batch_size, num_classes)\n",
    "        x = x * x_mask  # (batch_size, num_classes)\n",
    "        y_hats = x.max(dim=1)[1]  # (batch_size,)\n",
    "        accs = torch.eq(y_hats.long(), y.long()).float()  # (batch_size,)\n",
    "\n",
    "        x = x - x.max(dim=1, keepdim=True)[0]  # (batch_size, num_classes)\n",
    "        x = x * x_mask  # (batch_size, num_classes)\n",
    "        exp_x = torch.exp(x)  # (batch_size, num_classes)\n",
    "        exp_x = exp_x * x_mask  # (batch_size, num_classes)\n",
    "\n",
    "        sum_exp_x = exp_x.sum(dim=1)  # (batch_size,)\n",
    "        log_sum_exp_x = torch.log(sum_exp_x)  # (batch_size,)\n",
    "        x_star = x[torch.arange(0,x.size(0)).long(), y.data]  # (batch_size,)\n",
    "        xents = log_sum_exp_x - x_star  # (batch_size,)\n",
    "        return xents, accs, y_hats\n",
    "    \n",
    "    def _span_sums(self, stt, end, p_lens, max_p_len, batch_size, dim, max_ans_len):\n",
    "        max_ans_len_range = torch.arange(0,max_ans_len).unsqueeze(0)  # (1, max_ans_len)\n",
    "        offsets = torch.arange(0,max_p_len).unsqueeze(1)  # (max_p_len, 1)\n",
    "        end_idxs = max_ans_len_range + offsets  # (max_p_len, max_ans_len)\n",
    "        end_idxs_flat = end_idxs.view(-1).long()  # (max_p_len*max_ans_len,)\n",
    "        end_padded = torch.cat([end, Variable(torch.zeros(max_ans_len - 1, batch_size, dim))], 0)# (max_p_len+max_ans_len-1, batch_size, dim)\n",
    "        \n",
    "        end_structured = end_padded[end_idxs_flat]  # (max_p_len*max_ans_len, batch_size, dim)\n",
    "        \n",
    "        end_structured = end_structured.view(max_p_len, max_ans_len, batch_size, dim)  # (max_p_len, max_ans_len, batch_size, dim)\n",
    "        stt_shuffled = stt.unsqueeze(3).permute(0, 3, 1, 2)  # (max_p_len, 1, batch_size, dim)\n",
    "\n",
    "        span_sums = stt_shuffled + end_structured  # (max_p_len, max_ans_len, batch_size, dim)\n",
    "        span_sums_reshaped = span_sums.permute(2, 0, 1, 3).contiguous().view(batch_size, max_p_len * max_ans_len, dim) # (batch_size, max_p_len*max_ans_len, dim)\n",
    "\n",
    "        p_lens_shuffled = p_lens.unsqueeze(1)  # (batch_size, 1)\n",
    "        end_idxs_flat_shuffled = end_idxs_flat.unsqueeze(0)  # (1, max_p_len*max_ans_len)\n",
    "\n",
    "        span_masks_reshaped = torch.lt(end_idxs_flat_shuffled, p_lens_shuffled)  # (batch_size, max_p_len*max_ans_len)\n",
    "        span_masks_reshaped = span_masks_reshaped.float()\n",
    "\n",
    "        # (batch_size, max_p_len*max_ans_len, dim), (batch_size, max_p_len*max_ans_len)\n",
    "        return span_sums_reshaped, span_masks_reshaped\n",
    "    \n",
    "    def softmax_depths_with_mask(self,x, mask):\n",
    "        assert len(x.size()) == 3\n",
    "        assert len(mask.size()) == 3\n",
    "        # for numerical stability\n",
    "        x = x*mask\n",
    "        x = x - x.min(dim=2, keepdim=True)[0]\n",
    "        x = x*mask\n",
    "        x = x - x.max(dim=2, keepdim=True)[0]\n",
    "        e_x = mask * torch.exp(x)\n",
    "        sums = e_x.sum(dim=2, keepdim=True)\n",
    "        y = e_x / (sums + (torch.eq(sums, 0).float()))\n",
    "        y = y*mask\n",
    "        return y\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        with_bias = [self.linear1, self.linear_ans_start]\n",
    "        without_bias = [self.linear_w, self.linear_q_aligned, self.linear_ans_end, self.linear_span]\n",
    "\n",
    "        for layer in with_bias:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "            layer.bias.data.fill_(0)\n",
    "        for layer in without_bias:\n",
    "            layer.weight.data.uniform_(-initrange, initrange)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QAModel(word_emb,emb_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current batch Accuracy %=  0.0  --  Loss =  8.083648681640625\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.098410606384277\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.133763313293457\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.00380802154541\n",
      "Current batch Accuracy %=  0.0  --  Loss =  7.966842174530029\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.245445251464844\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.133466720581055\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.102537155151367\n",
      "Current batch Accuracy %=  0.0  --  Loss =  7.975963115692139\n",
      "Current batch Accuracy %=  0.0  --  Loss =  7.951416492462158\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.318339347839355\n",
      "Current batch Accuracy %=  0.0  --  Loss =  7.951519012451172\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.080277442932129\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.221004486083984\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.122572898864746\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.164862632751465\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.0526762008667\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.20546817779541\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.2205171585083\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.032506942749023\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.183837890625\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.04449462890625\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.071714401245117\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.196663856506348\n",
      "Current batch Accuracy %=  0.0  --  Loss =  7.988751411437988\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.16027545928955\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.127095222473145\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.251875877380371\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.022123336791992\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.152005195617676\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.194331169128418\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.161799430847168\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.041218757629395\n",
      "Current batch Accuracy %=  0.0  --  Loss =  7.961569309234619\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.072152137756348\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.073683738708496\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.068148612976074\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.143956184387207\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.081060409545898\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.223958015441895\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.065888404846191\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.165802001953125\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.12986946105957\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.10045051574707\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.25292682647705\n",
      "Current batch Accuracy %=  0.0  --  Loss =  7.987451553344727\n",
      "Current batch Accuracy %=  0.0  --  Loss =  7.842000484466553\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.151288032531738\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.018977165222168\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.117254257202148\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.064440727233887\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.0058012008667\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.159547805786133\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.055557250976562\n",
      "Current batch Accuracy %=  3.125  --  Loss =  8.063369750976562\n",
      "Current batch Accuracy %=  0.0  --  Loss =  7.9822187423706055\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.201544761657715\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.15605354309082\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.144875526428223\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.026116371154785\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.068429946899414\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.17979907989502\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.127297401428223\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.023453712463379\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.007887840270996\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.21407413482666\n",
      "Current batch Accuracy %=  0.0  --  Loss =  7.964100360870361\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.209328651428223\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.117944717407227\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.073751449584961\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.09337043762207\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.203889846801758\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.050483703613281\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.005263328552246\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.002846717834473\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.129090309143066\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.018060684204102\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.100138664245605\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.072880744934082\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.07975959777832\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.145094871520996\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.097286224365234\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.151707649230957\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.269104957580566\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.076216697692871\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.016157150268555\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.069991111755371\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.115148544311523\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.060861587524414\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.070061683654785\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.174830436706543\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.127494812011719\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.140937805175781\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.331645011901855\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.20035171508789\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.204439163208008\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.172417640686035\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.226004600524902\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.190803527832031\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.108150482177734\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.021773338317871\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.148126602172852\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.135163307189941\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.174944877624512\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.177042961120605\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.149567604064941\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.123650550842285\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.098113059997559\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.228516578674316\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.211977005004883\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.075575828552246\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.075037956237793\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.040210723876953\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.062904357910156\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.067138671875\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.229904174804688\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.164992332458496\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.115036010742188\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.13060188293457\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.167987823486328\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.060385704040527\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.245270729064941\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.095817565917969\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.001089096069336\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.23877239227295\n",
      "Current batch Accuracy %=  0.0  --  Loss =  7.957658767700195\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.214022636413574\n",
      "Current batch Accuracy %=  0.0  --  Loss =  7.9157185554504395\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.072454452514648\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.038806915283203\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.039143562316895\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.175719261169434\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.011299133300781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current batch Accuracy %=  3.125  --  Loss =  8.256509780883789\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.253328323364258\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.235136032104492\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.068202018737793\n",
      "Current batch Accuracy %=  0.0  --  Loss =  7.985485076904297\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.077609062194824\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.160874366760254\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.143902778625488\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.092930793762207\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.125333786010742\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.144758224487305\n",
      "Current batch Accuracy %=  0.0  --  Loss =  7.953866958618164\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.041092872619629\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.203978538513184\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.192593574523926\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.117537498474121\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.146221160888672\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.05688762664795\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.089652061462402\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.124156951904297\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.068760871887207\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.117478370666504\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.225330352783203\n",
      "Current batch Accuracy %=  0.0  --  Loss =  7.937082767486572\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.034235954284668\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.148212432861328\n",
      "Current batch Accuracy %=  3.125  --  Loss =  8.148186683654785\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.138117790222168\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.003317832946777\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.090270042419434\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.103277206420898\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.033400535583496\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.060588836669922\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.225414276123047\n",
      "Current batch Accuracy %=  0.0  --  Loss =  7.97843599319458\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.202300071716309\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.195934295654297\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.122400283813477\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.0752534866333\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.08863639831543\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.230796813964844\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.069172859191895\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.136241912841797\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.241886138916016\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.209869384765625\n",
      "Current batch Accuracy %=  0.0  --  Loss =  7.9995880126953125\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.229519844055176\n",
      "Current batch Accuracy %=  0.0  --  Loss =  8.002134323120117\n"
     ]
    }
   ],
   "source": [
    "x = 0\n",
    "train_loss_hist = []\n",
    "train_acc_hist = []\n",
    "for epoch in range(max_num_epochs):\n",
    "    loss_curr_epoch = 0.0\n",
    "    acc_curr_epoch = 0.0\n",
    "    n_done = 0\n",
    "    for data in train_data:\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        contexts = Variable(data[0])\n",
    "        n_done += contexts.size(1)\n",
    "        contexts_mask = Variable(data[1])\n",
    "        questions = Variable(data[2])\n",
    "        questions_mask = Variable(data[3])\n",
    "        anss = data[4]\n",
    "        contexts_lens = Variable(data[5])\n",
    "        y = Variable(data[6])\n",
    "        \n",
    "        loss, acc, sum_acc, sum_loss = model(contexts, contexts_mask, questions, questions_mask, y, contexts_lens)\n",
    "        \n",
    "        loss_curr_epoch += sum_loss\n",
    "        acc_curr_epoch += sum_acc\n",
    "        \n",
    "        print('Current batch Accuracy %= ', acc.data[0]*100,' --  Loss = ', loss.data[0])\n",
    "        logger.info('Current batch Accuracy %= ', acc.data[0]*100,' --  Loss = ', loss.data[0])\n",
    "        loss.backward()\n",
    "\n",
    "    current_epoch_train_loss = loss_curr_epoch/n_done\n",
    "    current_epoch_train_acc = acc_curr_epoch/n_done\n",
    "    print('End of Epoch'+(epoch+1)+'. Training Accuracy %= ', current_epoch_train_acc*100,' --  Loss = ', current_epoch_train_loss)\n",
    "    loss_hist.append(current_epoch_train_loss)\n",
    "    train_acc_hist.append(current_epoch_train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
